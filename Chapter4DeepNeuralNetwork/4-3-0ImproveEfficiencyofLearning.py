# 前節で、闇雲にニューロン/隠れ層を増やすというアプローチでは正しく学習できないことが発覚
# では具体的にどんな問題があるのか
# a. 勾配消失問題 - 勾配があっという間に0に近づいてしまい、誤差逆伝播法が機能しない
# b. 過学習 - 次元の高すぎる関数を用いることで、より複雑な訓練データの表現ができるが、実際のデータとは乖離し真の分布を求めることができない

# 以下のアプローチでそれらを解決しにいく
# a. 勾配消失問題
# 3-4-1-1活性化関数をtanhにする
# 3-4-1-2活性化関数をReLuにする
# 3-4-1-3活性化関数をLeakyReLuにする
# 3-4-1-4活性化関数をParametricReLuにする

# b. 過学習(オーバーフィッティング)
# 4-3-2ドロップアウト
